import fetch from 'node-fetch';
import dotenv from 'dotenv';

// Carregar variÃ¡veis de ambiente
dotenv.config();

console.log('ğŸ¤– === TESTANDO CONFIGURAÃ‡ÃƒO OLLAMA ===');
console.log('ğŸ¯ Verificando se Ollama estÃ¡ funcionando para chat');

const OLLAMA_URL = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'llama3.2:1b';

setTimeout(async () => {
  console.log('\n1ï¸âƒ£ === VERIFICANDO SE OLLAMA ESTÃ RODANDO ===');
  
  try {
    const healthResponse = await fetch(`${OLLAMA_URL}/api/tags`, {
      timeout: 5000
    });

    if (healthResponse.ok) {
      const data = await healthResponse.json();
      console.log('âœ… Ollama estÃ¡ rodando!');
      console.log('ğŸ“¦ Modelos disponÃ­veis:', data.models?.length || 0);
      
      if (data.models && data.models.length > 0) {
        data.models.forEach((model, index) => {
          console.log(`   ${index + 1}. ${model.name} (${(model.size / 1024 / 1024 / 1024).toFixed(1)}GB)`);
        });
      }
      
      // Verificar se o modelo configurado existe
      const hasModel = data.models?.some(m => m.name.includes(OLLAMA_MODEL.split(':')[0]));
      if (hasModel) {
        console.log(`âœ… Modelo ${OLLAMA_MODEL} encontrado!`);
      } else {
        console.log(`âš ï¸ Modelo ${OLLAMA_MODEL} nÃ£o encontrado`);
        console.log(`ğŸ’¡ Execute: ollama pull ${OLLAMA_MODEL}`);
      }
      
    } else {
      console.log('âŒ Ollama nÃ£o estÃ¡ respondendo');
    }
    
  } catch (error) {
    console.log('âŒ Ollama nÃ£o estÃ¡ rodando:', error.message);
    console.log('\nğŸ“‹ INSTRUÃ‡Ã•ES PARA INSTALAR:');
    console.log('1. Baixe: https://ollama.ai/download/windows');
    console.log('2. Execute: ollama pull llama3.2:1b');
    console.log('3. Teste: ollama run llama3.2:1b "OlÃ¡"');
    return;
  }
  
  console.log('\n2ï¸âƒ£ === TESTANDO CONVERSA COM IA ===');
  
  try {
    const prompt = 'OlÃ¡! Responda de forma breve e amigÃ¡vel.';
    
    const aiResponse = await fetch(`${OLLAMA_URL}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: OLLAMA_MODEL,
        prompt: prompt,
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 100,
          top_k: 40,
          top_p: 0.9
        }
      }),
      timeout: 30000
    });

    if (aiResponse.ok) {
      const aiData = await aiResponse.json();
      console.log('âœ… IA respondeu com sucesso!');
      console.log('ğŸ¤– Resposta:', aiData.response);
      console.log('âš¡ Tempo:', aiData.total_duration ? `${(aiData.total_duration / 1000000).toFixed(0)}ms` : 'N/A');
      console.log('ğŸ§  Tokens:', aiData.eval_count || 'N/A');
      
    } else {
      console.log('âŒ Erro na resposta da IA:', aiResponse.status);
    }
    
  } catch (error) {
    console.log('âŒ Erro ao testar IA:', error.message);
  }
  
  console.log('\n3ï¸âƒ£ === TESTANDO CHAT COMPLETO ===');
  
  try {
    // Simular uma conversa completa
    const chatResponse = await fetch('http://localhost:3001/api/conversations', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        walletAddress: 'GABRVJFNHAH4JMDDTEOJHRRYTVO7SAEW64G2O5UIVMA64AZTUISJP53E',
        title: 'Teste Ollama Local'
      })
    });

    if (chatResponse.ok) {
      const chatData = await chatResponse.json();
      const conversationId = chatData.conversation.id;
      
      console.log('âœ… Conversa criada:', conversationId);
      
      // Enviar mensagem
      const messageResponse = await fetch(`http://localhost:3001/api/conversations/${conversationId}/messages`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          walletAddress: 'GABRVJFNHAH4JMDDTEOJHRRYTVO7SAEW64G2O5UIVMA64AZTUISJP53E',
          message: 'OlÃ¡! Me conte uma curiosidade interessante em poucas palavras.',
          secretKey: 'SAWIQZJL2YPFWHP4XSKTLGWCZUW7MXWGLXMAY7STMHINCB6IXQZVXFWM'
        })
      });

      if (messageResponse.ok) {
        const messageData = await messageResponse.json();
        console.log('âœ… Chat funcionando com Ollama!');
        console.log('ğŸ’¬ Mensagens:', messageData.messages?.length || 0);
        console.log('ğŸ’° Custo:', messageData.cost, 'XLM');
        console.log('ğŸ”— TX Hash:', messageData.txHash?.substring(0, 12) + '...');
        
        if (messageData.messages && messageData.messages.length >= 2) {
          const aiMessage = messageData.messages[1];
          console.log('ğŸ¤– IA disse:', aiMessage.content.substring(0, 100) + '...');
        }
        
      } else {
        console.log('âŒ Erro no chat:', messageResponse.status);
      }
      
    } else {
      console.log('âŒ Erro ao criar conversa');
    }
    
  } catch (error) {
    console.log('âŒ Erro no teste de chat:', error.message);
  }
  
  console.log('\nğŸ‰ === RESULTADO FINAL ===');
  console.log('âœ… Se tudo funcionou, seu chat estÃ¡ pronto!');
  console.log('ğŸ’¬ Acesse: http://localhost:5173/studio');
  console.log('ğŸ¤– A IA local Ollama responderÃ¡ suas perguntas');
  console.log('ğŸ’° E debitarÃ¡ do seu saldo Stellar real!');
  
}, 2000);

console.log('\nğŸ“‹ VERIFICAÃ‡Ã•ES:');
console.log('ğŸ” 1. Ollama estÃ¡ instalado e rodando?');
console.log('ğŸ¤– 2. Modelo baixado e funcionando?');
console.log('ğŸ’¬ 3. Chat integrado com backend?');
console.log('ğŸ’° 4. Pagamento Stellar funcionando?');
console.log('â³ Aguardando 2 segundos...');
