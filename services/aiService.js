import axios from 'axios';
import dotenv from 'dotenv';

dotenv.config();

class AIService {
  constructor() {
    this.provider = process.env.AI_PROVIDER || 'ollama';
    this.ollamaBaseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    this.ollamaModel = process.env.OLLAMA_MODEL || 'llama3.2:1b';
    this.openaiApiBase = process.env.OPENAI_API_BASE;
    this.openaiApiKey = process.env.OPENAI_API_KEY;
    this.openaiModel = process.env.OPENAI_MODEL || 'gpt-3.5-turbo';
  }

  /**
   * Chama a IA com fallback autom√°tico
   * @param {string} prompt - O prompt para a IA
   * @returns {Promise<{response: string, executionTime: number}>}
   */
  async callAI(prompt) {
    const startTime = Date.now();
    
    console.log('ü§ñ === AI SERVICE - callAI ===');
    console.log('üîß Provider configurado:', this.provider);
    console.log('üåê Ollama URL:', this.ollamaBaseUrl);
    console.log('ü§ñ Ollama Model:', this.ollamaModel);
    console.log('üí¨ Prompt length:', prompt.length);
    
    try {
      // Tentar Ollama primeiro (modo padr√£o)
      if (this.provider === 'ollama') {
        console.log('üîÑ Chamando Ollama...');
        const response = await this.callOllama(prompt);
        console.log('‚úÖ Ollama respondeu com sucesso!');
        console.log('üìù Resposta length:', response.length);
        return {
          response,
          executionTime: Date.now() - startTime
        };
      }
      
      // Tentar OpenAI-compat√≠vel se configurado
      if (this.openaiApiBase && this.openaiApiKey) {
        const response = await this.callOpenAI(prompt);
        return {
          response,
          executionTime: Date.now() - startTime
        };
      }
      
      // Fallback para mock
      return this.callMock(prompt, startTime);
      
    } catch (error) {
      console.warn(`Erro no provider ${this.provider}:`, error.message);
      
      // Tentar fallbacks
      try {
        if (this.provider === 'ollama' && this.openaiApiBase && this.openaiApiKey) {
          console.log('üîÑ Tentando fallback para OpenAI...');
          const response = await this.callOpenAI(prompt);
          return {
            response,
            executionTime: Date.now() - startTime
          };
        }
      } catch (fallbackError) {
        console.warn('Erro no fallback OpenAI:', fallbackError.message);
      }
      
      // √öltimo recurso: mock
      console.log('üîÑ Usando resposta mock...');
      return this.callMock(prompt, startTime);
    }
  }

  /**
   * Chama o Ollama local
   * @param {string} prompt 
   * @returns {Promise<string>}
   */
  async callOllama(prompt) {
    // Melhorar o prompt para respostas diretas e relevantes
    const enhancedPrompt = `${prompt}

INSTRU√á√ïES:
- Responda de forma DIRETA e RELEVANTE √† pergunta
- Se pedirem uma resposta simples (como "ok"), responda exatamente isso
- Seja conciso e objetivo
- Use formata√ß√£o clara quando necess√°rio
- N√£o invente informa√ß√µes desnecess√°rias`;

    const response = await axios.post(`${this.ollamaBaseUrl}/api/generate`, {
      model: this.ollamaModel,
      prompt: enhancedPrompt,
      stream: false,
      options: {
        temperature: 0.3, // Mais determin√≠stico
        num_predict: 200, // Respostas mais curtas
        top_p: 0.8,
        repeat_penalty: 1.1,
        stop: ["\n\n\n"] // Parar em quebras excessivas
      }
    }, {
      timeout: 120000, // 2 minutos para Ollama
      headers: {
        'Content-Type': 'application/json'
      }
    });

    if (!response.data || !response.data.response) {
      throw new Error('Resposta inv√°lida do Ollama');
    }

    return response.data.response.trim();
  }

  /**
   * Chama API compat√≠vel com OpenAI
   * @param {string} prompt 
   * @returns {Promise<string>}
   */
  async callOpenAI(prompt) {
    const response = await axios.post(`${this.openaiApiBase}/chat/completions`, {
      model: this.openaiModel,
      messages: [
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: 500,
      temperature: 0.7
    }, {
      timeout: 30000,
      headers: {
        'Authorization': `Bearer ${this.openaiApiKey}`,
        'Content-Type': 'application/json'
      }
    });

    if (!response.data?.choices?.[0]?.message?.content) {
      throw new Error('Resposta inv√°lida da API OpenAI');
    }

    return response.data.choices[0].message.content.trim();
  }

  /**
   * Resposta mock para desenvolvimento/fallback
   * @param {string} prompt 
   * @param {number} startTime 
   * @returns {object}
   */
  callMock(prompt, startTime) {
    // Simular delay real√≠stico
    const mockDelay = Math.random() * 2000 + 1000; // 1-3 segundos
    
    return new Promise((resolve) => {
      setTimeout(() => {
        const responses = [
          `Esta √© uma resposta simulada para o seu prompt: "${prompt.substring(0, 50)}${prompt.length > 50 ? '...' : ''}"\n\nEm um ambiente de produ√ß√£o, esta resposta viria do modelo de IA configurado (Ollama ou OpenAI-compat√≠vel). Para ativar a IA real, configure as vari√°veis de ambiente apropriadas.`,
          
          `Ol√°! Recebi seu prompt sobre "${prompt.substring(0, 30)}${prompt.length > 30 ? '...' : ''}". Esta √© uma resposta de demonstra√ß√£o.\n\nPara respostas reais de IA:\n- Configure o Ollama localmente, ou\n- Configure uma API compat√≠vel com OpenAI\n\nConsulte o README para instru√ß√µes detalhadas.`,
          
          `Prompt processado com sucesso! ü§ñ\n\nSeu texto: "${prompt.substring(0, 40)}${prompt.length > 40 ? '...' : ''}"\n\nEsta √© uma simula√ß√£o. Para ativar IA real, verifique a configura√ß√£o dos servi√ßos de IA no arquivo .env do backend.`
        ];
        
        const randomResponse = responses[Math.floor(Math.random() * responses.length)];
        
        resolve({
          response: randomResponse,
          executionTime: Date.now() - startTime
        });
      }, mockDelay);
    });
  }

  /**
   * Verifica se algum provider est√° dispon√≠vel
   * @returns {Promise<{available: boolean, provider: string, status: string}>}
   */
  async checkAvailability() {
    // Testar Ollama
    try {
      const response = await axios.get(`${this.ollamaBaseUrl}/api/tags`, {
        timeout: 5000
      });
      
      if (response.status === 200) {
        return {
          available: true,
          provider: 'ollama',
          status: `Ollama dispon√≠vel com ${response.data.models?.length || 0} modelos`
        };
      }
    } catch (error) {
      console.log('Ollama n√£o dispon√≠vel:', error.message);
    }

    // Testar OpenAI-compat√≠vel
    if (this.openaiApiBase && this.openaiApiKey) {
      try {
        const response = await axios.get(`${this.openaiApiBase}/models`, {
          timeout: 5000,
          headers: {
            'Authorization': `Bearer ${this.openaiApiKey}`
          }
        });
        
        if (response.status === 200) {
          return {
            available: true,
            provider: 'openai',
            status: 'API OpenAI-compat√≠vel dispon√≠vel'
          };
        }
      } catch (error) {
        console.log('OpenAI-compat√≠vel n√£o dispon√≠vel:', error.message);
      }
    }

    return {
      available: false,
      provider: 'mock',
      status: 'Usando respostas simuladas - configure um provider de IA'
    };
  }
}

export default new AIService();
